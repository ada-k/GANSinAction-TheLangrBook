{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepCNN GAN - DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jz-kPIjhNv4"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/ada-k/GANSinAction-TheLangrBook/blob/main/gans_mnistGeneration.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPL3GXVlpll2"
      },
      "source": [
        "# cup of tea\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Activation, Dense, BatchNormalization, Dropout, Flatten, Reshape\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.datasets import mnist\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpEwrUjOS1S0"
      },
      "source": [
        "# input dimensions\n",
        "img_shape = (28, 28, 1)\n",
        "z_dim = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fjpliu0TBl9"
      },
      "source": [
        "# generator definition: transposed convolution\n",
        "def build_generator(z_dim):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Take a random noise vector and reshape it into a 7 × 7 × 256 tensor through afully connected layer\n",
        "    model.add(Dense(256 * 7 * 7, input_dim = z_dim))\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "\n",
        "    # Use transposed convolution, transforming the 7 × 7 × 256 tensor into a 14 × 14× 128 tensor\n",
        "    model.add(Conv2DTranspose(128, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "\n",
        "    #batch normalisation and leakyrelu activation\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha = 0.01))\n",
        "\n",
        "    # transforming the 14 × 14 × 128 tensor into a 14 ×14 × 64 tensor (h and w remain unchanged by setting stride = 1)\n",
        "    model.add(Conv2DTranspose(64, kernel_size = 3, strides = 1, padding = 'same'))\n",
        "\n",
        "    # batch normalisation and leakyrelu activation again\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha = 0.01))\n",
        "\n",
        "    # transforming the 14 × 14 × 64 tensor into the out-put image size, 28 × 28 × 1\n",
        "    model.add(Conv2DTranspose(1, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "\n",
        "    # tanh activation on output layer\n",
        "    model.add(Activation('tanh'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn8_PcMHYF0k"
      },
      "source": [
        "# build discriminator\n",
        "def build_discriminator(img_shape):\n",
        "    model = Sequential()\n",
        "    # Use a convolutional layer to transform a 28 × 28 × 1 input image into a 14 × 14 ×32 tensor\n",
        "    model.add(Conv2D(32, kernel_size = 3, strides = 2, input_shape = img_shape, padding = 'same'))\n",
        "\n",
        "    # apply the Leaky ReLU activation function\n",
        "    model.add(LeakyReLU(alpha = 0.01))\n",
        "\n",
        "    # Use a convolutional layer, transforming the 14 × 14 × 32 tensor into a 7 × 7 × 64tensor\n",
        "    model.add(Conv2D(64, kernel_size = 3, strides = 2, input_shape = img_shape, padding = 'same'))\n",
        "\n",
        "    # Apply batch normalization and the Leaky ReLU activation function\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha = 0.01))\n",
        "\n",
        "    # Use a convolutional layer, transforming the 7 × 7 × 64 tensor into a 3 × 3 × 128tensor\n",
        "    model.add(Conv2D(128, kernel_size = 3, strides = 2, input_shape = img_shape, padding = 'same'))\n",
        "\n",
        "    # Apply batch normalization and the Leaky ReLU activation function\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha = 0.01))\n",
        "\n",
        "    # Flatten the 3 × 3 × 128 tensor into a vector of size 3 × 3 × 128 = 1152\n",
        "    model.add(Flatten())\n",
        "    # output layer\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzUF8UM4aE1q",
        "outputId": "2abd7a67-3bc8-4b6c-82a9-5c989407feef"
      },
      "source": [
        "# build and run\n",
        "def build_gan(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "\n",
        "    return model\n",
        "\n",
        "# build and compile the discriminator\n",
        "discriminator = build_discriminator(img_shape)\n",
        "discriminator.compile(loss = 'binary_crossentropy', optimizer = Adam(), metrics = ['accuracy'])\n",
        "\n",
        "# build generator\n",
        "generator = build_generator(z_dim)\n",
        "\n",
        "discriminator.trainable = False # keep discriminator's parameters unchangeable\n",
        "\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss = 'binary_crossentropy', optimizer = Adam())\n",
        "gan.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_8 (Sequential)    (None, 28, 28, 1)         1637121   \n",
            "_________________________________________________________________\n",
            "sequential_7 (Sequential)    (None, 1)                 95489     \n",
            "=================================================================\n",
            "Total params: 1,732,610\n",
            "Trainable params: 1,636,737\n",
            "Non-trainable params: 95,873\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJxjQaYDeXzO"
      },
      "source": [
        "# display sample images\n",
        "def sample_images(generator, image_grid_rows=4, image_grid_columns=4):  \n",
        "    # random noise sample  \n",
        "    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))   \n",
        "    # x* - generate images from the noise \n",
        "    gen_imgs = generator.predict(z)   \n",
        "    # rescale images to [0, 1] pixel values      \n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5     \n",
        "    # img grid              \n",
        "    fig, axs = plt.subplots(image_grid_rows,                                  \n",
        "                            image_grid_columns,                            \n",
        "                            figsize=(4, 4),                            \n",
        "                            sharey=True,                            \n",
        "                            sharex=True)    \n",
        "    cnt = 0\n",
        "    for i in range(image_grid_rows):\n",
        "        for j in range(image_grid_columns):            \n",
        "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')  \n",
        "            plt.savefig('/content/sample_data/')  # write image to file\n",
        "            plt.cla()             \n",
        "            axs[i, j].axis('off')            \n",
        "            cnt += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38VmZw3CeKWg"
      },
      "source": [
        "# train our gan\n",
        "\n",
        "'''We get a ran-dom mini-batch of MNIST images as real examples and generate a mini-batch of fakeimages\n",
        " from random noise vectors z. We then use those to train the Discriminator net-work  while \n",
        "keeping  the  Generator’s  parameters  constant.  Next,  we  generate  a  mini-batch of fake \n",
        "  images and use those to train the Generator network while keeping theDiscriminator’s parameters fixed.\n",
        "   We repeat this for each iteration. To generatez, we sample from the standard normal distribution '''\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "\n",
        "def train(iterations, batch_size, sample_interval):\n",
        "\n",
        "    (x_train, _), (_, _) = mnist.load_data()\n",
        "    x_train = x_train/127.5 - 1.0 #rescale grayscale values(1, 255) to [-1, 1]\n",
        "    x_train = np.expand_dims(x_train, axis = 3)\n",
        "\n",
        "    real = np.ones((batch_size, 1)) # labels for real images\n",
        "    fake = np.zeros((batch_size, 1)) # fake images labels\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        idx = np.random.randint(0, x_train.shape[0], batch_size)       #random batch of real images     \n",
        "        imgs = x_train[idx]\n",
        "\n",
        "        #batch of fake images \n",
        "        z = np.random.normal(0, 1, (batch_size, 100))             \n",
        "        gen_imgs = generator.predict(z)\n",
        "        # trains discriminator\n",
        "        d_loss_real = discriminator.train_on_batch(imgs, real)            \n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)        \n",
        "        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # generate batch of fake images\n",
        "        z = np.random.normal(0, 1, (batch_size, 100))\n",
        "        # train generators           \n",
        "        gen_imgs = generator.predict(z)        \n",
        "        g_loss = gan.train_on_batch(z, real)\n",
        "\n",
        "        # save losses and accuracies for plotting\n",
        "        if (iteration + 1) % sample_interval == 0:            \n",
        "            losses.append((d_loss, g_loss))                    \n",
        "            accuracies.append(100.0 * accuracy)            \n",
        "            iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "        # output training progress\n",
        "        print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration + 1, d_loss, 100.0 * accuracy, g_loss)) \n",
        "        # output sample images           \n",
        "        sample_images(generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_ujwxiiehfj",
        "outputId": "505a4460-176c-44da-dc10-cfc49062ff7e"
      },
      "source": [
        "iterations = 20000           \n",
        "batch_size = 128\n",
        "sample_interval = 1000\n",
        "\n",
        "train(iterations, batch_size, sample_interval)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 [D loss: 1.097088, acc.: 30.86%] [G loss: 0.683705]\n",
            "2 [D loss: 0.877438, acc.: 50.78%] [G loss: 0.688056]\n",
            "3 [D loss: 0.517644, acc.: 64.06%] [G loss: 0.710748]\n",
            "4 [D loss: 0.113600, acc.: 99.61%] [G loss: 0.741198]\n",
            "5 [D loss: 0.042242, acc.: 100.00%] [G loss: 0.762275]\n",
            "6 [D loss: 0.095548, acc.: 97.27%] [G loss: 0.768528]\n",
            "7 [D loss: 0.109643, acc.: 96.88%] [G loss: 0.760631]\n",
            "8 [D loss: 0.048985, acc.: 99.61%] [G loss: 0.749632]\n",
            "9 [D loss: 0.028780, acc.: 99.61%] [G loss: 0.741368]\n",
            "10 [D loss: 0.013284, acc.: 100.00%] [G loss: 0.730226]\n",
            "11 [D loss: 0.014109, acc.: 99.61%] [G loss: 0.720760]\n",
            "12 [D loss: 0.015090, acc.: 100.00%] [G loss: 0.714766]\n",
            "13 [D loss: 0.012650, acc.: 100.00%] [G loss: 0.701385]\n",
            "14 [D loss: 0.011662, acc.: 100.00%] [G loss: 0.696538]\n",
            "15 [D loss: 0.010940, acc.: 100.00%] [G loss: 0.693448]\n",
            "16 [D loss: 0.010737, acc.: 100.00%] [G loss: 0.691579]\n",
            "17 [D loss: 0.006759, acc.: 100.00%] [G loss: 0.690873]\n",
            "18 [D loss: 0.007796, acc.: 100.00%] [G loss: 0.695606]\n",
            "19 [D loss: 0.011939, acc.: 100.00%] [G loss: 0.702541]\n",
            "20 [D loss: 0.012220, acc.: 100.00%] [G loss: 0.716986]\n",
            "21 [D loss: 0.014910, acc.: 100.00%] [G loss: 0.740786]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22 [D loss: 0.009803, acc.: 100.00%] [G loss: 0.777085]\n",
            "23 [D loss: 0.005595, acc.: 100.00%] [G loss: 0.810905]\n",
            "24 [D loss: 0.006287, acc.: 100.00%] [G loss: 0.839552]\n",
            "25 [D loss: 0.006915, acc.: 100.00%] [G loss: 0.862583]\n",
            "26 [D loss: 0.008919, acc.: 100.00%] [G loss: 0.873566]\n",
            "27 [D loss: 0.018565, acc.: 99.61%] [G loss: 0.877693]\n",
            "28 [D loss: 0.018479, acc.: 100.00%] [G loss: 0.887155]\n",
            "29 [D loss: 0.012471, acc.: 100.00%] [G loss: 0.898683]\n",
            "30 [D loss: 0.009110, acc.: 100.00%] [G loss: 0.911254]\n",
            "31 [D loss: 0.009706, acc.: 100.00%] [G loss: 0.918581]\n",
            "32 [D loss: 0.009989, acc.: 100.00%] [G loss: 0.917851]\n",
            "33 [D loss: 0.009478, acc.: 100.00%] [G loss: 0.913583]\n",
            "34 [D loss: 0.010985, acc.: 100.00%] [G loss: 0.912990]\n",
            "35 [D loss: 0.009762, acc.: 100.00%] [G loss: 0.927585]\n",
            "36 [D loss: 0.008296, acc.: 100.00%] [G loss: 0.933788]\n",
            "37 [D loss: 0.009663, acc.: 100.00%] [G loss: 0.930868]\n",
            "38 [D loss: 0.010780, acc.: 100.00%] [G loss: 0.920524]\n",
            "39 [D loss: 0.007594, acc.: 100.00%] [G loss: 0.930896]\n",
            "40 [D loss: 0.007755, acc.: 100.00%] [G loss: 0.925376]\n",
            "41 [D loss: 0.005769, acc.: 100.00%] [G loss: 0.919932]\n",
            "42 [D loss: 0.006786, acc.: 100.00%] [G loss: 0.908772]\n",
            "43 [D loss: 0.007486, acc.: 100.00%] [G loss: 0.906018]\n",
            "44 [D loss: 0.006832, acc.: 100.00%] [G loss: 0.908756]\n",
            "45 [D loss: 0.006433, acc.: 100.00%] [G loss: 0.907377]\n",
            "46 [D loss: 0.011010, acc.: 100.00%] [G loss: 0.899136]\n",
            "47 [D loss: 0.008802, acc.: 100.00%] [G loss: 0.904811]\n",
            "48 [D loss: 0.013323, acc.: 100.00%] [G loss: 0.905430]\n",
            "49 [D loss: 0.011055, acc.: 100.00%] [G loss: 0.915861]\n",
            "50 [D loss: 0.007355, acc.: 100.00%] [G loss: 0.926041]\n",
            "51 [D loss: 0.010584, acc.: 100.00%] [G loss: 0.928999]\n",
            "52 [D loss: 0.008327, acc.: 100.00%] [G loss: 0.936897]\n",
            "53 [D loss: 0.008380, acc.: 100.00%] [G loss: 0.954121]\n",
            "54 [D loss: 0.016284, acc.: 99.61%] [G loss: 0.960587]\n",
            "55 [D loss: 0.020734, acc.: 100.00%] [G loss: 1.009860]\n",
            "56 [D loss: 0.008157, acc.: 100.00%] [G loss: 1.060627]\n",
            "57 [D loss: 0.015216, acc.: 100.00%] [G loss: 1.068413]\n",
            "58 [D loss: 0.011098, acc.: 100.00%] [G loss: 1.103495]\n",
            "59 [D loss: 0.008400, acc.: 100.00%] [G loss: 1.166514]\n",
            "60 [D loss: 0.002839, acc.: 100.00%] [G loss: 1.236563]\n",
            "61 [D loss: 0.007080, acc.: 100.00%] [G loss: 1.277285]\n",
            "62 [D loss: 0.011976, acc.: 99.61%] [G loss: 1.274147]\n",
            "63 [D loss: 0.006883, acc.: 100.00%] [G loss: 1.316393]\n",
            "64 [D loss: 0.002647, acc.: 100.00%] [G loss: 1.372405]\n",
            "65 [D loss: 0.002454, acc.: 100.00%] [G loss: 1.409815]\n",
            "66 [D loss: 0.003173, acc.: 100.00%] [G loss: 1.430489]\n",
            "67 [D loss: 0.002487, acc.: 100.00%] [G loss: 1.410008]\n",
            "68 [D loss: 0.002005, acc.: 100.00%] [G loss: 1.412138]\n",
            "69 [D loss: 0.001792, acc.: 100.00%] [G loss: 1.392455]\n",
            "70 [D loss: 0.002238, acc.: 100.00%] [G loss: 1.389972]\n",
            "71 [D loss: 0.002721, acc.: 100.00%] [G loss: 1.386476]\n",
            "72 [D loss: 0.002491, acc.: 100.00%] [G loss: 1.395878]\n",
            "73 [D loss: 0.002748, acc.: 100.00%] [G loss: 1.395942]\n",
            "74 [D loss: 0.002555, acc.: 100.00%] [G loss: 1.391807]\n",
            "75 [D loss: 0.002098, acc.: 100.00%] [G loss: 1.384933]\n",
            "76 [D loss: 0.003052, acc.: 100.00%] [G loss: 1.372951]\n",
            "77 [D loss: 0.006019, acc.: 100.00%] [G loss: 1.325991]\n",
            "78 [D loss: 0.005572, acc.: 100.00%] [G loss: 1.309697]\n",
            "79 [D loss: 0.006950, acc.: 100.00%] [G loss: 1.325919]\n",
            "80 [D loss: 0.006252, acc.: 100.00%] [G loss: 1.348108]\n",
            "81 [D loss: 0.003507, acc.: 100.00%] [G loss: 1.375139]\n",
            "82 [D loss: 0.003816, acc.: 100.00%] [G loss: 1.373044]\n",
            "83 [D loss: 0.004026, acc.: 100.00%] [G loss: 1.347421]\n",
            "84 [D loss: 0.003557, acc.: 100.00%] [G loss: 1.340533]\n",
            "85 [D loss: 0.003986, acc.: 100.00%] [G loss: 1.316742]\n",
            "86 [D loss: 0.002352, acc.: 100.00%] [G loss: 1.314355]\n",
            "87 [D loss: 0.001283, acc.: 100.00%] [G loss: 1.322182]\n",
            "88 [D loss: 0.001245, acc.: 100.00%] [G loss: 1.302435]\n",
            "89 [D loss: 0.001803, acc.: 100.00%] [G loss: 1.282293]\n",
            "90 [D loss: 0.002244, acc.: 100.00%] [G loss: 1.220196]\n",
            "91 [D loss: 0.001024, acc.: 100.00%] [G loss: 1.173413]\n",
            "92 [D loss: 0.000836, acc.: 100.00%] [G loss: 1.120235]\n",
            "93 [D loss: 0.001045, acc.: 100.00%] [G loss: 1.070717]\n",
            "94 [D loss: 0.000878, acc.: 100.00%] [G loss: 1.031853]\n",
            "95 [D loss: 0.000703, acc.: 100.00%] [G loss: 0.988695]\n",
            "96 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.962999]\n",
            "97 [D loss: 0.000666, acc.: 100.00%] [G loss: 0.934700]\n",
            "98 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.902830]\n",
            "99 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.870482]\n",
            "100 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.847006]\n",
            "101 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.825767]\n",
            "102 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.796603]\n",
            "103 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.762064]\n",
            "104 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.740261]\n",
            "105 [D loss: 0.000497, acc.: 100.00%] [G loss: 0.709210]\n",
            "106 [D loss: 0.000881, acc.: 100.00%] [G loss: 0.699956]\n",
            "107 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.677333]\n",
            "108 [D loss: 0.000588, acc.: 100.00%] [G loss: 0.646567]\n",
            "109 [D loss: 0.000991, acc.: 100.00%] [G loss: 0.635144]\n",
            "110 [D loss: 0.000613, acc.: 100.00%] [G loss: 0.618488]\n",
            "111 [D loss: 0.000672, acc.: 100.00%] [G loss: 0.594572]\n",
            "112 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.586686]\n",
            "113 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.569050]\n",
            "114 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.563417]\n",
            "115 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.551834]\n",
            "116 [D loss: 0.000447, acc.: 100.00%] [G loss: 0.538980]\n",
            "117 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.534438]\n",
            "118 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.528954]\n",
            "119 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.520492]\n",
            "120 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.517018]\n",
            "121 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.498543]\n",
            "122 [D loss: 0.000569, acc.: 100.00%] [G loss: 0.499198]\n",
            "123 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.497517]\n",
            "124 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.493675]\n",
            "125 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.499698]\n",
            "126 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.502395]\n",
            "127 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.509899]\n",
            "128 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.513675]\n",
            "129 [D loss: 0.000449, acc.: 100.00%] [G loss: 0.511358]\n",
            "130 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.512261]\n",
            "131 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.503202]\n",
            "132 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.489630]\n",
            "133 [D loss: 0.000418, acc.: 100.00%] [G loss: 0.482793]\n",
            "134 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.460883]\n",
            "135 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.456181]\n",
            "136 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.444327]\n",
            "137 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.436770]\n",
            "138 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.434641]\n",
            "139 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.424454]\n",
            "140 [D loss: 0.000533, acc.: 100.00%] [G loss: 0.421648]\n",
            "141 [D loss: 0.000498, acc.: 100.00%] [G loss: 0.425492]\n",
            "142 [D loss: 0.000438, acc.: 100.00%] [G loss: 0.422112]\n",
            "143 [D loss: 0.000497, acc.: 100.00%] [G loss: 0.423035]\n",
            "144 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.420890]\n",
            "145 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.430906]\n",
            "146 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.428755]\n",
            "147 [D loss: 0.000399, acc.: 100.00%] [G loss: 0.429454]\n",
            "148 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.439199]\n",
            "149 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.440600]\n",
            "150 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.439454]\n",
            "151 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.436910]\n",
            "152 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.428701]\n",
            "153 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.422795]\n",
            "154 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.419299]\n",
            "155 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.414939]\n",
            "156 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.403390]\n",
            "157 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.392930]\n",
            "158 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.385606]\n",
            "159 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.376666]\n",
            "160 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.360414]\n",
            "161 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.346002]\n",
            "162 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.327676]\n",
            "163 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.312405]\n",
            "164 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.290923]\n",
            "165 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.273527]\n",
            "166 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.257694]\n",
            "167 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.237416]\n",
            "168 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.227632]\n",
            "169 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.217639]\n",
            "170 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.210366]\n",
            "171 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.203802]\n",
            "172 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.200735]\n",
            "173 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.194188]\n",
            "174 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.189515]\n",
            "175 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.182814]\n",
            "176 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.177940]\n",
            "177 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.168584]\n",
            "178 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.161186]\n",
            "179 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.155392]\n",
            "180 [D loss: 0.000428, acc.: 100.00%] [G loss: 0.152136]\n",
            "181 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.148330]\n",
            "182 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.146810]\n",
            "183 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.144413]\n",
            "184 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.139528]\n",
            "185 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.140671]\n",
            "186 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.137672]\n",
            "187 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.137028]\n",
            "188 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.135415]\n",
            "189 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.132961]\n",
            "190 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.132025]\n",
            "191 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.132660]\n",
            "192 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.131390]\n",
            "193 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.129313]\n",
            "194 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.126513]\n",
            "195 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.125375]\n",
            "196 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.123232]\n",
            "197 [D loss: 0.000649, acc.: 100.00%] [G loss: 0.117661]\n",
            "198 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.108992]\n",
            "199 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.102721]\n",
            "200 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.096504]\n",
            "201 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.095387]\n",
            "202 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.091568]\n",
            "203 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.089853]\n",
            "204 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.089113]\n",
            "205 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.087586]\n",
            "206 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.085927]\n",
            "207 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.084705]\n",
            "208 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.083419]\n",
            "209 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.080181]\n",
            "210 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.076189]\n",
            "211 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.072656]\n",
            "212 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.068865]\n",
            "213 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.066272]\n",
            "214 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.062691]\n",
            "215 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.059250]\n",
            "216 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.055973]\n",
            "217 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.054119]\n",
            "218 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.052679]\n",
            "219 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.052012]\n",
            "220 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.049828]\n",
            "221 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.048185]\n",
            "222 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.049899]\n",
            "223 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.048141]\n",
            "224 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.045897]\n",
            "225 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.044617]\n",
            "226 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.043926]\n",
            "227 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.042449]\n",
            "228 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.041131]\n",
            "229 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.040063]\n",
            "230 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.038536]\n",
            "231 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.037373]\n",
            "232 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.036422]\n",
            "233 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.035745]\n",
            "234 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.034576]\n",
            "235 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.034611]\n",
            "236 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.034074]\n",
            "237 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.033314]\n",
            "238 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.032740]\n",
            "239 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.032071]\n",
            "240 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.031071]\n",
            "241 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.030954]\n",
            "242 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.029901]\n",
            "243 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.029444]\n",
            "244 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.029027]\n",
            "245 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.028514]\n",
            "246 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.028409]\n",
            "247 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.027106]\n",
            "248 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.026489]\n",
            "249 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.025902]\n",
            "250 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.025196]\n",
            "251 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.024138]\n",
            "252 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.023085]\n",
            "253 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.022007]\n",
            "254 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.021047]\n",
            "255 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.019847]\n",
            "256 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.018887]\n",
            "257 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.017910]\n",
            "258 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.017030]\n",
            "259 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.016341]\n",
            "260 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.015667]\n",
            "261 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.015365]\n",
            "262 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.014901]\n",
            "263 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.014620]\n",
            "264 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.014464]\n",
            "265 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.014087]\n",
            "266 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.014184]\n",
            "267 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.013803]\n",
            "268 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.013843]\n",
            "269 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.013432]\n",
            "270 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.013294]\n",
            "271 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.013084]\n",
            "272 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.012903]\n",
            "273 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.012729]\n",
            "274 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.012457]\n",
            "275 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.012291]\n",
            "276 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.012204]\n",
            "277 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.011983]\n",
            "278 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.011732]\n",
            "279 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.011614]\n",
            "280 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.011500]\n",
            "281 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.011241]\n",
            "282 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.011274]\n",
            "283 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.011450]\n",
            "284 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.011254]\n",
            "285 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.011308]\n",
            "286 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.011247]\n",
            "287 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.011374]\n",
            "288 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.011052]\n",
            "289 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.011006]\n",
            "290 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.011186]\n",
            "291 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.011260]\n",
            "292 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.011073]\n",
            "293 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.011079]\n",
            "294 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.011238]\n",
            "295 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.011275]\n",
            "296 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.011211]\n",
            "297 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.011370]\n",
            "298 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.011506]\n",
            "299 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.011500]\n",
            "300 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.011508]\n",
            "301 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.011627]\n",
            "302 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.011312]\n",
            "303 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.011476]\n",
            "304 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.011664]\n",
            "305 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.011551]\n",
            "306 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.011804]\n",
            "307 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.011896]\n",
            "308 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.011882]\n",
            "309 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.012042]\n",
            "310 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.012058]\n",
            "311 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.012111]\n",
            "312 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.011945]\n",
            "313 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.012207]\n",
            "314 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.012111]\n",
            "315 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.012176]\n",
            "316 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.012394]\n",
            "317 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.012296]\n",
            "318 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.012456]\n",
            "319 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.012531]\n",
            "320 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.012677]\n",
            "321 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.012574]\n",
            "322 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.012681]\n",
            "323 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.012692]\n",
            "324 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.012605]\n",
            "325 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.012628]\n",
            "326 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.012719]\n",
            "327 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.012486]\n",
            "328 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.012366]\n",
            "329 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.012315]\n",
            "330 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.012227]\n",
            "331 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.012036]\n",
            "332 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.012126]\n",
            "333 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.011938]\n",
            "334 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.011823]\n",
            "335 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.011822]\n",
            "336 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.011695]\n",
            "337 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.011894]\n",
            "338 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.011754]\n",
            "339 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.011733]\n",
            "340 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.011615]\n",
            "341 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.011637]\n",
            "342 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.011554]\n",
            "343 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.011392]\n",
            "344 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.011294]\n",
            "345 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.011186]\n",
            "346 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.010958]\n",
            "347 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.010646]\n",
            "348 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.010687]\n",
            "349 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.010326]\n",
            "350 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.010048]\n",
            "351 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.009839]\n",
            "352 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.009831]\n",
            "353 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.009641]\n",
            "354 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.009250]\n",
            "355 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.009060]\n",
            "356 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.008886]\n",
            "357 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.008554]\n",
            "358 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.008535]\n",
            "359 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.008250]\n",
            "360 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.008029]\n",
            "361 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.007943]\n",
            "362 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.007789]\n",
            "363 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.007682]\n",
            "364 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.007419]\n",
            "365 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.007398]\n",
            "366 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.007286]\n",
            "367 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.007043]\n",
            "368 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.006976]\n",
            "369 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.006948]\n",
            "370 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.006905]\n",
            "371 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.006868]\n",
            "372 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.006785]\n",
            "373 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.006820]\n",
            "374 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.006673]\n",
            "375 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.006711]\n",
            "376 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.006857]\n",
            "377 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.006969]\n",
            "378 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.007134]\n",
            "379 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.007072]\n",
            "380 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.007120]\n",
            "381 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.007278]\n",
            "382 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.007274]\n",
            "383 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.007326]\n",
            "384 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.007306]\n",
            "385 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.007372]\n",
            "386 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.007294]\n",
            "387 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.007310]\n",
            "388 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.007229]\n",
            "389 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.007126]\n",
            "390 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.007109]\n",
            "391 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.006997]\n",
            "392 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.006890]\n",
            "393 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.006982]\n",
            "394 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.006889]\n",
            "395 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.006699]\n",
            "396 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.006515]\n",
            "397 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.006534]\n",
            "398 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.006411]\n",
            "399 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.006221]\n",
            "400 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.006200]\n",
            "401 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.006126]\n",
            "402 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.006153]\n",
            "403 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.006038]\n",
            "404 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.005913]\n",
            "405 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.005768]\n",
            "406 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.005661]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}